{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/tony/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuations = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower() \n",
    "    text = text.replace(punctuations, \" \") \n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    text = \" \".join(words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    df = pd.read_csv(filename, sep='\\t', names=['text', 'label'])\n",
    "    df['text'] = df['text'].apply(preprocess)\n",
    "    return df['text'].values, df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_data():\n",
    "    X_train, y_train = load_data('NLP_ass_train.tsv')\n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = load_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['cannot continue calling feminists rights womxn arent addressed yes sexual offences public list trans lesbian bisexual queer womxn able enter information reporting sheet gender forum',\n",
       "       'nawt yall niggers ignoring',\n",
       "       '<user> bit confused coz chinese ppl access twitter thn ching chong using think pakistani 🤔 🤔 🤔',\n",
       "       ...,\n",
       "       'macht der moslem wenn der zion gegen seinen propheten hetzt machst du wenn die roten ratten gegen deinen toten opa hetzen',\n",
       "       'awful look world demographics asians fucking everywhere another betrayal white genocide real',\n",
       "       'jewish globalist elite imported million muslims multiculturalize weaken america right violence lgbt black street criminals sometimes lgbt random muslim grow think start seeing organized attacks'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-31 17:32:26.974115: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-08-31 17:32:27.071025: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-08-31 17:32:28.452961: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# from gensim.models import Word2Vec\n",
    "import gensim.models.keyedvectors as word2vec\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load word2vec model\n",
    "# word2vec_model = Word2Vec.load(\"GoogleNews-vectors-negative300.bin\")\n",
    "word2vec_model = word2vec.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert text to word2vec vectors by averaging word vectors\n",
    "def text_to_word2vec(text):\n",
    "    words = text.split()\n",
    "    word_vectors = []\n",
    "    for word in words:\n",
    "        if word in word2vec_model:\n",
    "            word_vectors.append(word2vec_model[word])\n",
    "\n",
    "    if len(word_vectors) > 0:\n",
    "        return np.mean(word_vectors, axis=0)\n",
    "    else: \n",
    "        return np.zeros(300) # vector of zeros for oov words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate word2vec vectors for all texts in training data \n",
    "X_train_vectors = []\n",
    "for text in X_train:\n",
    "    X_train_vectors.append(text_to_word2vec(text)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vectors = np.array(X_train_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.03108726e-01,  1.88954677e-02,  1.76269531e-01,  1.09049477e-01,\n",
       "        1.17187500e-02, -4.64680977e-02, -1.45833328e-01, -5.02522774e-02,\n",
       "        5.78613281e-02,  1.80013016e-01, -3.40169258e-02, -2.08007812e-01,\n",
       "       -3.47391772e-03,  5.34261055e-02, -1.73502609e-01, -6.51041651e-03,\n",
       "        3.38541679e-02, -8.49609375e-02,  1.10677080e-02, -1.39648438e-01,\n",
       "        2.42024735e-01, -1.33666992e-02,  2.05078125e-01, -2.49674484e-01,\n",
       "       -3.12500000e-02,  7.50325546e-02, -1.52384445e-01, -2.91341152e-02,\n",
       "        4.99674492e-02, -7.46256486e-02, -2.31119785e-02, -1.12304688e-02,\n",
       "       -3.16406250e-01, -1.24918623e-02, -1.51529953e-01,  3.79231758e-02,\n",
       "        1.03841148e-01,  3.66210938e-04,  1.21744789e-01,  1.95312500e-01,\n",
       "        1.18448891e-01, -5.84309883e-02,  3.25846344e-01, -8.63240585e-02,\n",
       "        3.09244785e-02, -1.29313156e-01, -1.40462235e-01, -2.54231781e-01,\n",
       "       -1.63330078e-01,  1.27604172e-01, -8.97623673e-02,  2.08007812e-01,\n",
       "       -2.52278652e-02,  1.38631180e-01, -6.21744804e-02,  1.29882812e-01,\n",
       "       -1.23779297e-01, -5.63151054e-02,  2.66927093e-01, -8.98437500e-02,\n",
       "        1.19547524e-01,  3.61328125e-01,  9.31599960e-02, -6.65690079e-02,\n",
       "        4.67020683e-02,  4.10156250e-02, -1.20035805e-01,  5.80647774e-02,\n",
       "       -1.16282143e-01,  8.17057267e-02,  1.04003906e-01,  2.25585938e-01,\n",
       "       -4.02018242e-02,  1.23565674e-01, -3.13476562e-01, -2.81168614e-02,\n",
       "       -8.78906250e-03, -4.20735665e-02,  4.41080742e-02,  1.37451172e-01,\n",
       "       -3.41145843e-01,  1.43391922e-01, -1.72363281e-01, -1.43554688e-01,\n",
       "        3.09244799e-03,  3.25520843e-04, -1.57877609e-01,  2.49837235e-01,\n",
       "        9.08203125e-02, -4.20939140e-02, -2.15087891e-01,  1.67968750e-01,\n",
       "        9.43705216e-02, -1.24023438e-01, -6.58493042e-02,  9.67610702e-02,\n",
       "        1.82332352e-01, -6.49414062e-02,  3.07617188e-01, -2.84423828e-01,\n",
       "       -2.06420898e-01, -1.84977219e-01,  2.15169266e-01, -1.29882812e-01,\n",
       "        7.46968612e-02, -3.51969413e-02, -2.04671219e-01,  5.68033867e-02,\n",
       "        6.93359375e-02, -1.86360683e-02, -3.07942718e-01, -1.94173172e-01,\n",
       "       -2.44344082e-02,  1.93354294e-01, -6.73421239e-03, -2.57568359e-02,\n",
       "        1.33951828e-01, -1.85546875e-02, -1.05794273e-01, -1.23046875e-01,\n",
       "       -4.22526032e-01,  8.16243514e-02, -2.26236984e-01, -9.30989608e-02,\n",
       "        2.50651032e-01, -2.52278638e-03, -2.35758469e-01,  3.17993164e-02,\n",
       "        8.13802052e-03, -2.87760407e-01, -2.56347656e-01, -1.27766922e-01,\n",
       "       -4.74446602e-02,  5.50702401e-02,  5.64982109e-02,  4.96419258e-02,\n",
       "        7.05973292e-03,  1.49637863e-01,  1.88313797e-01, -5.07812500e-02,\n",
       "        2.36409500e-01, -1.35416672e-01,  1.26139328e-01, -5.89192696e-02,\n",
       "       -1.62679031e-01,  1.01725258e-04,  1.79036462e-03, -3.01595062e-01,\n",
       "        1.33951828e-01, -4.98046875e-02,  2.46744797e-01, -6.02213526e-03,\n",
       "        4.59798193e-03, -2.84505218e-01, -4.31315117e-02,  1.25569656e-01,\n",
       "       -1.15234375e-01,  1.60156250e-01,  2.53906250e-02, -2.51057949e-02,\n",
       "       -1.46158859e-01,  1.12141930e-01, -2.56347656e-02,  8.21889266e-02,\n",
       "       -4.96419286e-03, -3.29915375e-01,  3.25358063e-01, -1.05794268e-02,\n",
       "       -9.94059220e-02,  6.51092529e-02, -1.39414474e-01,  2.30183914e-01,\n",
       "        1.44368485e-01,  1.21927895e-01,  2.60416674e-03, -8.52864608e-02,\n",
       "        2.46744797e-01, -3.30078125e-01,  3.58072929e-02,  4.48404960e-02,\n",
       "       -2.73437500e-02, -8.63443986e-02,  3.28776054e-02,  1.26627609e-01,\n",
       "       -8.73616561e-02,  3.56038399e-02, -6.51041651e-03, -4.60611992e-02,\n",
       "        1.37695312e-01,  2.28841141e-01,  1.02172852e-01,  3.65397148e-02,\n",
       "       -1.87174473e-02,  8.78906250e-02,  6.69352189e-02,  1.68294266e-01,\n",
       "       -1.69067383e-01, -3.31217460e-02, -1.30126953e-01, -5.74544258e-02,\n",
       "       -1.96451828e-01,  1.02945967e-02, -5.32226562e-02, -1.86116531e-01,\n",
       "       -8.72192383e-02, -4.15039062e-03,  5.09033203e-02, -9.64355469e-02,\n",
       "       -2.43326817e-02,  1.29740402e-01, -2.17692051e-02,  7.91015625e-02,\n",
       "        1.33280441e-01, -1.46886185e-01,  1.28580734e-01, -6.34765625e-03,\n",
       "        4.62646484e-02, -2.26562500e-01, -1.61214188e-01, -1.20442711e-01,\n",
       "       -1.72119141e-01,  3.39192718e-01, -1.52994795e-02,  1.29394531e-01,\n",
       "        1.13606773e-01, -7.03125000e-02,  1.11979164e-01,  1.93094894e-01,\n",
       "       -8.60595703e-02,  2.92968750e-03,  4.06901054e-02, -9.53776017e-02,\n",
       "        6.17675781e-02,  1.51448563e-01,  8.01188126e-02, -3.45052071e-02,\n",
       "        2.84179688e-01,  1.16373701e-02,  1.55436203e-01, -1.06811523e-01,\n",
       "        2.57768005e-01,  1.11409508e-01, -9.52148438e-02, -1.57389328e-01,\n",
       "       -1.02376305e-01,  1.57552078e-01, -1.89208984e-02,  3.79394531e-01,\n",
       "       -3.62955742e-02, -1.40787764e-02,  2.98502594e-01,  2.48860672e-01,\n",
       "        3.42610665e-02,  1.92057297e-01,  2.16893509e-01, -2.31770828e-01,\n",
       "       -5.19205742e-02,  2.18912754e-02,  1.05957031e-01,  1.73746739e-02,\n",
       "       -7.38118514e-02, -4.80143214e-03,  5.67626953e-02,  6.07910156e-02,\n",
       "       -1.22151695e-01, -5.04557304e-02, -2.20286045e-02,  5.37109375e-03,\n",
       "       -1.58203125e-01, -6.28255233e-02, -2.18078613e-01,  1.33834839e-01,\n",
       "       -3.14394645e-02,  1.35742188e-01,  4.79024239e-02, -1.01745605e-01,\n",
       "       -1.36474609e-01, -3.45052071e-02, -1.77327469e-01, -6.99869767e-02,\n",
       "        7.94270858e-02, -1.91894531e-01, -1.15478516e-01,  1.25000000e-01,\n",
       "       -2.45361328e-01, -5.43619804e-02,  3.61328125e-02,  1.96940109e-01,\n",
       "        8.83992538e-02, -1.94173172e-01,  4.96927910e-02,  2.26033524e-01,\n",
       "       -2.76947021e-01, -1.79036462e-03, -2.30468750e-01, -1.56850174e-01,\n",
       "       -6.46158829e-02, -1.47786453e-01,  2.28474941e-02, -6.44531250e-02])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vectors[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['normal', 'normal', 'hatespeech', ..., 'normal', 'hatespeech',\n",
       "       'offensive'], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Fit and transform the labels\n",
    "y_train_encoded = le.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 0, 2, 0, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_encoded[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "# Convert labels to one-hot vectors\n",
    "y_train_one_hot = to_categorical(y_train_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_one_hot[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tony/miniconda3/envs/nlp-a2/lib/python3.9/site-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2025-08-31 17:32:50.930169: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2025-08-31 17:32:50.930259: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:171] verbose logging is disabled. Rerun with verbose logging (usually --v=1 or --vmodule=cuda_diagnostics=1) to get more diagnostic output from this module\n",
      "2025-08-31 17:32:50.930267: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:176] retrieving CUDA diagnostic information for host: fedora\n",
      "2025-08-31 17:32:50.930271: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:183] hostname: fedora\n",
      "2025-08-31 17:32:50.930613: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:190] libcuda reported version is: 575.64.5\n",
      "2025-08-31 17:32:50.930640: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:194] kernel reported version is: 575.64.5\n",
      "2025-08-31 17:32:50.930644: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:284] kernel version seems to match DSO: 575.64.5\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dropout, BatchNormalization\n",
    "\n",
    "# Initialize the model\n",
    "model = Sequential()\n",
    "\n",
    "# Add first dense layer with 256 neurons and 'relu' activation function\n",
    "model.add(Dense(256, activation='relu', input_dim=X_train_vectors.shape[1]))\n",
    "\n",
    "# Add batch normalization layer\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Add dropout layer\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Add second dense layer with 128 neurons and 'relu' activation function\n",
    "model.add(Dense(128, activation='relu'))\n",
    "\n",
    "# Add batch normalization layer\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Add dropout layer\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Add third dense layer with 64 neurons and 'relu' activation function\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Add batch normalization layer\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Add dropout layer\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Add output layer with 3 neurons (for 3 classes) and 'softmax' activation function\n",
    "model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'], run_eagerly=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 66ms/step - accuracy: 0.4017 - loss: 1.5540 - val_accuracy: 0.3864 - val_loss: 1.1544\n",
      "Epoch 2/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 64ms/step - accuracy: 0.5219 - loss: 1.0680 - val_accuracy: 0.4153 - val_loss: 1.1236\n",
      "Epoch 3/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 63ms/step - accuracy: 0.5730 - loss: 0.9321 - val_accuracy: 0.4257 - val_loss: 1.0957\n",
      "Epoch 4/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 64ms/step - accuracy: 0.6069 - loss: 0.8916 - val_accuracy: 0.4482 - val_loss: 1.0618\n",
      "Epoch 5/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 64ms/step - accuracy: 0.6207 - loss: 0.8596 - val_accuracy: 0.4657 - val_loss: 1.0625\n",
      "Epoch 6/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 68ms/step - accuracy: 0.6259 - loss: 0.8524 - val_accuracy: 0.4722 - val_loss: 1.0501\n",
      "Epoch 7/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 64ms/step - accuracy: 0.6256 - loss: 0.8440 - val_accuracy: 0.4696 - val_loss: 1.0502\n",
      "Epoch 8/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 65ms/step - accuracy: 0.6311 - loss: 0.8482 - val_accuracy: 0.4651 - val_loss: 1.0585\n",
      "Epoch 9/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 65ms/step - accuracy: 0.6462 - loss: 0.8245 - val_accuracy: 0.4768 - val_loss: 1.0457\n",
      "Epoch 10/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 64ms/step - accuracy: 0.6502 - loss: 0.8174 - val_accuracy: 0.4800 - val_loss: 1.0532\n",
      "Epoch 11/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 63ms/step - accuracy: 0.6510 - loss: 0.8053 - val_accuracy: 0.4800 - val_loss: 1.0568\n",
      "Epoch 12/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 55ms/step - accuracy: 0.6499 - loss: 0.8115 - val_accuracy: 0.4937 - val_loss: 1.0416\n",
      "Epoch 13/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 60ms/step - accuracy: 0.6572 - loss: 0.7943 - val_accuracy: 0.4716 - val_loss: 1.0790\n",
      "Epoch 14/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 64ms/step - accuracy: 0.6673 - loss: 0.7806 - val_accuracy: 0.4839 - val_loss: 1.0400\n",
      "Epoch 15/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 65ms/step - accuracy: 0.6695 - loss: 0.7687 - val_accuracy: 0.4933 - val_loss: 1.0564\n",
      "Epoch 16/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 61ms/step - accuracy: 0.6740 - loss: 0.7759 - val_accuracy: 0.5024 - val_loss: 1.0619\n",
      "Epoch 17/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 77ms/step - accuracy: 0.6734 - loss: 0.7702 - val_accuracy: 0.5008 - val_loss: 1.0344\n",
      "Epoch 18/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 74ms/step - accuracy: 0.6730 - loss: 0.7532 - val_accuracy: 0.4829 - val_loss: 1.0886\n",
      "Epoch 19/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 70ms/step - accuracy: 0.6845 - loss: 0.7389 - val_accuracy: 0.4859 - val_loss: 1.0725\n",
      "Epoch 20/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 73ms/step - accuracy: 0.6748 - loss: 0.7532 - val_accuracy: 0.4813 - val_loss: 1.0904\n",
      "Epoch 21/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 70ms/step - accuracy: 0.6897 - loss: 0.7306 - val_accuracy: 0.5024 - val_loss: 1.0650\n",
      "Epoch 22/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 60ms/step - accuracy: 0.6893 - loss: 0.7382 - val_accuracy: 0.5112 - val_loss: 1.0219\n",
      "Epoch 23/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 62ms/step - accuracy: 0.7061 - loss: 0.7065 - val_accuracy: 0.5011 - val_loss: 1.0474\n",
      "Epoch 24/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 66ms/step - accuracy: 0.7066 - loss: 0.6988 - val_accuracy: 0.4963 - val_loss: 1.0636\n",
      "Epoch 25/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 63ms/step - accuracy: 0.7026 - loss: 0.7056 - val_accuracy: 0.4940 - val_loss: 1.0917\n",
      "Epoch 26/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 64ms/step - accuracy: 0.7044 - loss: 0.6960 - val_accuracy: 0.5005 - val_loss: 1.0408\n",
      "Epoch 27/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 59ms/step - accuracy: 0.7087 - loss: 0.7025 - val_accuracy: 0.5031 - val_loss: 1.0928\n",
      "Epoch 28/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 55ms/step - accuracy: 0.7021 - loss: 0.6987 - val_accuracy: 0.4930 - val_loss: 1.0875\n",
      "Epoch 29/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - accuracy: 0.7207 - loss: 0.6721 - val_accuracy: 0.5138 - val_loss: 1.0594\n",
      "Epoch 30/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 62ms/step - accuracy: 0.7151 - loss: 0.6609 - val_accuracy: 0.4953 - val_loss: 1.1022\n",
      "Epoch 31/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 62ms/step - accuracy: 0.7130 - loss: 0.6740 - val_accuracy: 0.4849 - val_loss: 1.1430\n",
      "Epoch 32/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 55ms/step - accuracy: 0.7201 - loss: 0.6635 - val_accuracy: 0.4911 - val_loss: 1.1041\n",
      "Epoch 33/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 57ms/step - accuracy: 0.7353 - loss: 0.6437 - val_accuracy: 0.5190 - val_loss: 1.0738\n",
      "Epoch 34/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 55ms/step - accuracy: 0.7363 - loss: 0.6404 - val_accuracy: 0.4946 - val_loss: 1.1607\n",
      "Epoch 35/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 55ms/step - accuracy: 0.7368 - loss: 0.6380 - val_accuracy: 0.5021 - val_loss: 1.1128\n",
      "Epoch 36/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 56ms/step - accuracy: 0.7422 - loss: 0.6198 - val_accuracy: 0.5011 - val_loss: 1.0990\n",
      "Epoch 37/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 60ms/step - accuracy: 0.7471 - loss: 0.6208 - val_accuracy: 0.5037 - val_loss: 1.1292\n",
      "Epoch 38/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 62ms/step - accuracy: 0.7465 - loss: 0.6111 - val_accuracy: 0.4849 - val_loss: 1.1761\n",
      "Epoch 39/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 61ms/step - accuracy: 0.7531 - loss: 0.6008 - val_accuracy: 0.5044 - val_loss: 1.1490\n",
      "Epoch 40/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 59ms/step - accuracy: 0.7556 - loss: 0.6095 - val_accuracy: 0.4985 - val_loss: 1.1494\n",
      "Epoch 41/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 59ms/step - accuracy: 0.7577 - loss: 0.5843 - val_accuracy: 0.5122 - val_loss: 1.1516\n",
      "Epoch 42/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 60ms/step - accuracy: 0.7527 - loss: 0.5875 - val_accuracy: 0.4881 - val_loss: 1.2023\n",
      "Epoch 43/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 60ms/step - accuracy: 0.7665 - loss: 0.5754 - val_accuracy: 0.5167 - val_loss: 1.1372\n",
      "Epoch 44/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 59ms/step - accuracy: 0.7630 - loss: 0.5769 - val_accuracy: 0.4920 - val_loss: 1.2364\n",
      "Epoch 45/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 61ms/step - accuracy: 0.7627 - loss: 0.5755 - val_accuracy: 0.5054 - val_loss: 1.1785\n",
      "Epoch 46/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 60ms/step - accuracy: 0.7735 - loss: 0.5674 - val_accuracy: 0.5021 - val_loss: 1.2029\n",
      "Epoch 47/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 62ms/step - accuracy: 0.7773 - loss: 0.5485 - val_accuracy: 0.5018 - val_loss: 1.2285\n",
      "Epoch 48/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 63ms/step - accuracy: 0.7813 - loss: 0.5439 - val_accuracy: 0.5073 - val_loss: 1.2135\n",
      "Epoch 49/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 63ms/step - accuracy: 0.7888 - loss: 0.5184 - val_accuracy: 0.4823 - val_loss: 1.2568\n",
      "Epoch 50/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 61ms/step - accuracy: 0.7865 - loss: 0.5262 - val_accuracy: 0.4992 - val_loss: 1.2685\n",
      "Epoch 51/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 61ms/step - accuracy: 0.7910 - loss: 0.5202 - val_accuracy: 0.4771 - val_loss: 1.3648\n",
      "Epoch 52/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 60ms/step - accuracy: 0.7924 - loss: 0.5307 - val_accuracy: 0.4745 - val_loss: 1.3930\n",
      "Epoch 53/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 63ms/step - accuracy: 0.7865 - loss: 0.5264 - val_accuracy: 0.4979 - val_loss: 1.2796\n",
      "Epoch 54/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 60ms/step - accuracy: 0.7933 - loss: 0.5225 - val_accuracy: 0.5050 - val_loss: 1.2534\n",
      "Epoch 55/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 63ms/step - accuracy: 0.8072 - loss: 0.4937 - val_accuracy: 0.4846 - val_loss: 1.3054\n",
      "Epoch 56/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 62ms/step - accuracy: 0.8105 - loss: 0.4957 - val_accuracy: 0.4888 - val_loss: 1.3384\n",
      "Epoch 57/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 60ms/step - accuracy: 0.8038 - loss: 0.4948 - val_accuracy: 0.4930 - val_loss: 1.2811\n",
      "Epoch 58/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 64ms/step - accuracy: 0.8093 - loss: 0.4826 - val_accuracy: 0.4865 - val_loss: 1.3140\n",
      "Epoch 59/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 61ms/step - accuracy: 0.8091 - loss: 0.4853 - val_accuracy: 0.4992 - val_loss: 1.2683\n",
      "Epoch 60/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 61ms/step - accuracy: 0.8094 - loss: 0.4799 - val_accuracy: 0.5021 - val_loss: 1.2437\n",
      "Epoch 61/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 65ms/step - accuracy: 0.8117 - loss: 0.4735 - val_accuracy: 0.4985 - val_loss: 1.3173\n",
      "Epoch 62/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 63ms/step - accuracy: 0.8149 - loss: 0.4592 - val_accuracy: 0.4904 - val_loss: 1.3811\n",
      "Epoch 63/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 61ms/step - accuracy: 0.8167 - loss: 0.4646 - val_accuracy: 0.5018 - val_loss: 1.3012\n",
      "Epoch 64/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 65ms/step - accuracy: 0.8227 - loss: 0.4502 - val_accuracy: 0.4943 - val_loss: 1.3410\n",
      "Epoch 65/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 66ms/step - accuracy: 0.8251 - loss: 0.4458 - val_accuracy: 0.5034 - val_loss: 1.3017\n",
      "Epoch 66/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 63ms/step - accuracy: 0.8269 - loss: 0.4543 - val_accuracy: 0.4959 - val_loss: 1.3414\n",
      "Epoch 67/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 66ms/step - accuracy: 0.8176 - loss: 0.4609 - val_accuracy: 0.5073 - val_loss: 1.3185\n",
      "Epoch 68/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 67ms/step - accuracy: 0.8287 - loss: 0.4371 - val_accuracy: 0.5041 - val_loss: 1.3443\n",
      "Epoch 69/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 60ms/step - accuracy: 0.8228 - loss: 0.4584 - val_accuracy: 0.5034 - val_loss: 1.3653\n",
      "Epoch 70/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 57ms/step - accuracy: 0.8315 - loss: 0.4436 - val_accuracy: 0.4943 - val_loss: 1.3883\n",
      "Epoch 71/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 61ms/step - accuracy: 0.8210 - loss: 0.4485 - val_accuracy: 0.4898 - val_loss: 1.4091\n",
      "Epoch 72/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 61ms/step - accuracy: 0.8294 - loss: 0.4411 - val_accuracy: 0.4888 - val_loss: 1.4087\n",
      "Epoch 73/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 63ms/step - accuracy: 0.8346 - loss: 0.4236 - val_accuracy: 0.4836 - val_loss: 1.4777\n",
      "Epoch 74/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 62ms/step - accuracy: 0.8287 - loss: 0.4291 - val_accuracy: 0.5018 - val_loss: 1.3403\n",
      "Epoch 75/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 61ms/step - accuracy: 0.8433 - loss: 0.4129 - val_accuracy: 0.4894 - val_loss: 1.4263\n",
      "Epoch 76/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 60ms/step - accuracy: 0.8323 - loss: 0.4200 - val_accuracy: 0.5044 - val_loss: 1.4417\n",
      "Epoch 77/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 61ms/step - accuracy: 0.8338 - loss: 0.4190 - val_accuracy: 0.4927 - val_loss: 1.4268\n",
      "Epoch 78/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 62ms/step - accuracy: 0.8291 - loss: 0.4320 - val_accuracy: 0.4963 - val_loss: 1.4108\n",
      "Epoch 79/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 62ms/step - accuracy: 0.8427 - loss: 0.4012 - val_accuracy: 0.5034 - val_loss: 1.3792\n",
      "Epoch 80/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 66ms/step - accuracy: 0.8381 - loss: 0.4069 - val_accuracy: 0.4820 - val_loss: 1.4512\n",
      "Epoch 81/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 61ms/step - accuracy: 0.8398 - loss: 0.4087 - val_accuracy: 0.4976 - val_loss: 1.4271\n",
      "Epoch 82/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 63ms/step - accuracy: 0.8387 - loss: 0.4115 - val_accuracy: 0.4690 - val_loss: 1.5821\n",
      "Epoch 83/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 62ms/step - accuracy: 0.8522 - loss: 0.3880 - val_accuracy: 0.4953 - val_loss: 1.4563\n",
      "Epoch 84/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 61ms/step - accuracy: 0.8437 - loss: 0.4019 - val_accuracy: 0.4969 - val_loss: 1.4601\n",
      "Epoch 85/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 62ms/step - accuracy: 0.8474 - loss: 0.3889 - val_accuracy: 0.4885 - val_loss: 1.5119\n",
      "Epoch 86/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 61ms/step - accuracy: 0.8434 - loss: 0.3987 - val_accuracy: 0.4794 - val_loss: 1.6169\n",
      "Epoch 87/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 63ms/step - accuracy: 0.8454 - loss: 0.3948 - val_accuracy: 0.5021 - val_loss: 1.4342\n",
      "Epoch 88/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 65ms/step - accuracy: 0.8506 - loss: 0.3873 - val_accuracy: 0.4881 - val_loss: 1.5268\n",
      "Epoch 89/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 65ms/step - accuracy: 0.8520 - loss: 0.3834 - val_accuracy: 0.4953 - val_loss: 1.4696\n",
      "Epoch 90/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 67ms/step - accuracy: 0.8583 - loss: 0.3770 - val_accuracy: 0.4924 - val_loss: 1.4790\n",
      "Epoch 91/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 68ms/step - accuracy: 0.8598 - loss: 0.3627 - val_accuracy: 0.4963 - val_loss: 1.4912\n",
      "Epoch 92/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 65ms/step - accuracy: 0.8574 - loss: 0.3739 - val_accuracy: 0.5005 - val_loss: 1.4936\n",
      "Epoch 93/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 66ms/step - accuracy: 0.8594 - loss: 0.3641 - val_accuracy: 0.4901 - val_loss: 1.5534\n",
      "Epoch 94/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 66ms/step - accuracy: 0.8654 - loss: 0.3652 - val_accuracy: 0.4907 - val_loss: 1.5060\n",
      "Epoch 95/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 62ms/step - accuracy: 0.8541 - loss: 0.3675 - val_accuracy: 0.4914 - val_loss: 1.5146\n",
      "Epoch 96/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 67ms/step - accuracy: 0.8638 - loss: 0.3568 - val_accuracy: 0.4781 - val_loss: 1.5780\n",
      "Epoch 97/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 65ms/step - accuracy: 0.8569 - loss: 0.3734 - val_accuracy: 0.4855 - val_loss: 1.5557\n",
      "Epoch 98/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 64ms/step - accuracy: 0.8600 - loss: 0.3668 - val_accuracy: 0.4930 - val_loss: 1.5298\n",
      "Epoch 99/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 65ms/step - accuracy: 0.8627 - loss: 0.3547 - val_accuracy: 0.4885 - val_loss: 1.5450\n",
      "Epoch 100/100\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 70ms/step - accuracy: 0.8655 - loss: 0.3429 - val_accuracy: 0.4969 - val_loss: 1.5564\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f4afd9722e0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the input shape\n",
    "input_shape = (None, X_train_vectors.shape[1])\n",
    "\n",
    "# Build the model\n",
    "model.build(input_shape)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_vectors, y_train_one_hot, \n",
    "          epochs=100, \n",
    "          batch_size=64,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, y_val = load_data(filename=\"NLP_ass_valid.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate word2vec vectors for all texts in validation data \n",
    "X_val_vectors = []\n",
    "for text in X_val:\n",
    "    X_val_vectors.append(text_to_word2vec(text)) \n",
    "\n",
    "X_val_vectors = np.array(X_val_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_encoded = le.fit_transform(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "# Convert labels to one-hot vectors\n",
    "y_val_one_hot = to_categorical(y_val_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       ...,\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.6374 - loss: 1.1060\n",
      "Validation Accuracy: 0.5874089598655701\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on validation data\n",
    "loss, accuracy = model.evaluate(X_val_vectors, y_val_one_hot)\n",
    "print(\"Validation Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "Validation Accuracy: 58.74%\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "y_pred_prob = model.predict(X_val_vectors)\n",
    "\n",
    "# Convert probabilities into class labels\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_val_encoded, y_pred)\n",
    "\n",
    "print(f'Validation Accuracy: {accuracy * 100:.2f}%') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = load_data(\"NLP_ass_test.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate word2vec vectors for all texts in validation data \n",
    "X_test_vectors = []\n",
    "for text in X_test:\n",
    "    X_test_vectors.append(text_to_word2vec(text)) \n",
    "\n",
    "X_test_vectors = np.array(X_test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_encoded = le.fit_transform(y_test)\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Convert labels to one-hot vectors\n",
    "y_test_one_hot = to_categorical(y_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "Validation Accuracy: 59.30%\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "y_pred_prob = model.predict(X_test_vectors)\n",
    "\n",
    "# Convert probabilities into class labels\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test_encoded, y_pred)\n",
    "\n",
    "print(f'Validation Accuracy: {accuracy * 100:.2f}%') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-a2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
